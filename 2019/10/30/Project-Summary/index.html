<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><title>Project-Summary</title><link rel="stylesheet" href="css/normalize.css"><link rel="stylesheet" href="/css/hexo-theme-adoubi.css"><link rel="icon" href="/images/favicon.ico"><meta name="generator" content="Hexo 4.0.0"></head><body><div class="header"><a class="github-link"><img class="github-image" src="/images/Aquicon-Github-small.png"></a><a class="email-link" href="mailto:null" target="_blank" rel="noopener"><img class="email-image" src="/images/email-small.png"></a><a class="subscribe-link" href="/atom.xml"><img class="subscribe-image" src="/images/subscribe-small.png"></a></div><div class="content"><div class="post-item"></div><h2 class="post-title-wrapper"><p class="post-title">Project-Summary</p></h2><div class="post-date"><time datetime="2019-10-30T05:56:33.000Z">2019-10-30</time></div><div class="post-content"><h2 id="Home-Credit-Default-Risk"><a href="#Home-Credit-Default-Risk" class="headerlink" title="Home Credit Default Risk"></a>Home Credit Default Risk</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Many people struggle to get loans due to insufficient credit histories. In order to ensure that clients capable of repayment are given loans, which can help them to be successful, I utilize a variety of alternative data–including telco and transactional information–to predict clients’ repayment abilities. This is a supervised two-class classification problem of machine learning, and my project aims to predict whether banks should give loans to the client.</p>
<h3 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h3><h4 id="Data-balance"><a href="#Data-balance" class="headerlink" title="Data balance"></a>Data balance</h4><p>I split the “train” file in to train set and test set to validate the model. I randomly select 80 percent of rows with target 1 in “train” file and equivalent amount of rows with target 0 “train” file to constitute the train set. The test set consists of 20 percent remaining rows with target 1 and equivalent rows with target 0. This process serves to eliminate the possible influence of biases.</p>
<p><img src="/2019/10/30/Project-Summary/1.png" alt="1"></p>
<h4 id="Remove-bad-features-columns-and-records-row"><a href="#Remove-bad-features-columns-and-records-row" class="headerlink" title="Remove bad features (columns) and records (row)"></a>Remove bad features (columns) and records (row)</h4><p>I eliminate the rows and columns which have more than 40 percent blank space, because they have little value for reference. </p>
<p><img src="/2019/10/30/Project-Summary/2.png" alt="2"></p>
<h3 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h3><p>Data exploration and visualization serves to represent messy data on graphs, which enable people to see the correlation between data clearly and directly. Therefore, we can analyze patterns on the graphs and forecast future trends of certain events. If the data in the column do not have distinct influence on the TARGET column, I will delete this column. This process aims to determine which column needs to be included in feature engineering.</p>
<p><img src="/2019/10/30/Project-Summary/3.png" alt="3"></p>
<p>From the plot education and income, we can see that higher the level of education the client achieved, more income the client received.</p>
<p><img src="/2019/10/30/Project-Summary/4.png" alt="4"></p>
<p>From the plot income type (hue TARGET), we can see that people who receive income from working may be more difficult to get the loan in the further.</p>
<p><img src="/2019/10/30/Project-Summary/5.png" alt="5"></p>
<p>From the plot family status (hue TARGET), we can see that people who are married or widows may be easier to get the loan in the further.</p>
<p><img src="/2019/10/30/Project-Summary/6.png" alt="6"></p>
<p>From the plot housing type (hue TARGET), we can see that people who own apartments may be more capable of getting loans.</p>
<p><img src="/2019/10/30/Project-Summary/7.png" alt="7"></p>
<p>From the plot occupation type (hue TARGET), we can see that laborers are the least convincing among all the occupations.</p>
<p><img src="/2019/10/30/Project-Summary/8.png" alt="8"></p>
<p>From the plot, we can see that the day of the week which the client apply for the loan does not influence too much. </p>
<p><img src="/2019/10/30/Project-Summary/9.png" alt="9"></p>
<p>From the graph, we can discover that all the people in the sample provide mobile phone numbers, so it would not be an indicator of whether the banks should lend money to them.</p>
<h3 id="Feature-engineering"><a href="#Feature-engineering" class="headerlink" title="Feature engineering"></a>Feature engineering</h3><h4 id="Integer-encoding-and-one-hot-encoding"><a href="#Integer-encoding-and-one-hot-encoding" class="headerlink" title="Integer encoding and one-hot encoding"></a>Integer encoding and one-hot encoding</h4><p>Feature engineering is the process of using domain knowledge of the data to create or modify features to make machine-learning algorithms work. Feature engineering has two techniques: integer encoding, which is an algorithm mapping the category variable with integer, and one-hot encoding, which utilizes high (1) bits and low bits (0) to represent the data. In statistics, dummy variables represent a similar technique for representing categorical data. One-hot encoding is easy to design and modify. In addition, using a one-hot implementation typically allows a state machine to run at a faster clock rate than any other encoding of that state machine. However, one-hot encoding requires more storage than other encodings, and it may loss some information in some circumstance. It does not work effectively with too many categories. Through feature engineering, we can extract features from raw data to fit in algorithms and models. These features can make the data more understandable to machines and improve the function of the algorithms and models. If the data has increase or decrease by degrees, we usually use integer encoding. Otherwise, it is proper to use one-hot encoding.</p>
<h4 id="Grouping"><a href="#Grouping" class="headerlink" title="Grouping"></a>Grouping</h4><p>Grouping is the process of combining or dividing primary data into groups based on specific criteria. We can transform data into numerical data, using 0 or 1 to represent the data, and categorical data, classifying data into different categories. The grouping of data enables us to discover certain data distribution characteristics, which facilitates us to explore the correlation between two sets of data. Besides, the computer can better understand these data after grouping.</p>
<h4 id="Data-reduction"><a href="#Data-reduction" class="headerlink" title="Data reduction"></a>Data reduction</h4><p>Data reduction is the transformation of numerical or alphabetical digital information derived empirically or experimentally into a corrected, ordered, and simplified form. The basic concept is the reduction of multitudinous amounts of data down to the meaningful parts.</p>
<h3 id="Models-and-results"><a href="#Models-and-results" class="headerlink" title="Models and results"></a>Models and results</h3><h4 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h4><p>Logistic regression is a machine learning model for binary variable.</p>
<p><img src="/2019/10/30/Project-Summary/10.png" alt="10"></p>
<h4 id="Recursive-Feature-Elimination-RFE"><a href="#Recursive-Feature-Elimination-RFE" class="headerlink" title="Recursive Feature Elimination (RFE)"></a>Recursive Feature Elimination (RFE)</h4><p>Recursive feature elimination is based on the idea to repeatedly construct a model (for example an SVM or a regression model) and choose either the best or worst performing feature (for example based on coefficients), setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. Features are then ranked according to when they were eliminated. As such, it is a greedy optimization for finding the best performing subset of features.</p>
<p><img src="/2019/10/30/Project-Summary/11.png" alt="11"></p>
<h4 id="Principal-component-analysis-PCA"><a href="#Principal-component-analysis-PCA" class="headerlink" title="Principal component analysis (PCA)"></a>Principal component analysis (PCA)</h4><p>Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components.</p>
<h4 id="Optimization-of-Principal-component-analysis-PCA-Model"><a href="#Optimization-of-Principal-component-analysis-PCA-Model" class="headerlink" title="Optimization of Principal component analysis (PCA) Model"></a>Optimization of Principal component analysis (PCA) Model</h4><p>I changes the n_components of PCA model from 1 to 30 to see the result of accuracy. In the first plot below, x-axis is the n_components of PCA model and y-axis shows the accuracy. We can see from the plat that the accuracy increases until n_components=9 and the accuracy is almost same when n_components is higher than 9. In addition, when intercept_scaling=1000, the accuracy reaches its maximum of 60.429%.</p>
<p><img src="/2019/10/30/Project-Summary/12.png" alt="12"></p>
<p><img src="/2019/10/30/Project-Summary/13.png" alt="13"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>In this project, I predict whether banks should lend money to clients. I remove bad columns and rows, and reduce the features of the model. Finally I used logistic regression with dimensional reduction principal component analysis (n_components=9, intercept_scaling=1000) to do the prediction. The accuracy of the final model is 60.429%. Principal component analysis (PCA) improves the accuracy of the model. However, recursive feature elimination does not help with the accuracy of the model, but I believe that changing the parameters in recursive feature elimination (RFE) model may bring some improvements.</p>
</div></div><div class="footer"><div class="footer-copyright">Theme By <a href="https://github.com/shinux/hexo-theme-adoubi" target="_blank" rel="noopener">Adoubi</a> , Powered By Hexo.</div></div></body></html>